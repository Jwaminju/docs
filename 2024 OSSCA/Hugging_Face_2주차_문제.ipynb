{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jwaminju/docs/blob/main/2024%20OSSCA/Hugging_Face_2%EC%A3%BC%EC%B0%A8_%EB%AC%B8%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Transformers 실습하며 배우기 🤗"
      ],
      "metadata": {
        "id": "MgdWlhtgkspP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "작성자: Hugging Face KREW, [김하림](https://github.com/harheem)"
      ],
      "metadata": {
        "id": "aNrBlQ8AwHQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "트랜스포머 기반의 사전 학습 모델의 뛰어난 점은 엄청난 양의 텍스트 데이터를 사용해 단어와 문장의 관계를 스스로 학습한다는 점이에요. 책, 뉴스, 웹 문서 등 다양한 텍스트를 분석하며 언어의 구조와 의미를 이해하게 된답니다.\n",
        "\n",
        "하지만 이렇게 학습한 모델을 특정 작업에 활용하려면 추가 학습이 필요할 수 있어요. 이를 '파인튜닝'이라고 합니다. 파인튜닝은 적은 양의 labeled 데이터를 사용해 모델을 우리가 원하는 작업에 맞게 조정하는 과정을 의미해요.\n",
        "\n",
        "이번 주에는 이러한 파인튜닝을 위한 데이터셋을 어떻게 준비하는지 배워보도록 할게요. 좋은 데이터셋을 만드는 것이 정확한 모델을 얻는 첫 걸음이니까요 🤗"
      ],
      "metadata": {
        "id": "5f8g1LLZpRT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### #문제 004\n",
        "\n",
        "[IMDB 데이터세트](https://huggingface.co/datasets/stanfordnlp/imdb)는 영화 리뷰 코멘트의 긍정적/부정적 감성을 판단하기 위해 사용하는 감성 분석 데이터세트입니다. 데이터세트 구성은 다음과 같아요.\n",
        "- 25,000개 학습 데이터 (텍스트 및 레이블)\n",
        "- 25,000개 테스트 데이터 (텍스트 및 레이블)\n",
        "\n",
        "이 50,000개 데이터세트를 다운로드하는 코드를 작성해보세요. 그리고 학습과 테스트를 위해 랜덤하게 1,000개씩 데이터를 추출해보세요."
      ],
      "metadata": {
        "id": "cOXvKkjPk1Fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 HINT\n",
        "\n",
        "1.\t필요한 라이브러리 설치:\n",
        "\n",
        "  먼저, Hugging Face의 🤗 datasets 라이브러리를 설치해야 합니다. 커맨드 라인에서 다음 명령어를 실행하세요.\n",
        "\n",
        "  `!pip install datasets`\n",
        "2. `load_dataset()`을 이용하여 데이터세트 불러오세요.\n",
        "\n",
        "  ```python\n",
        "    from datasets import load_dataset\n",
        "    dataset = load_dataset('여기에 데이터세트 이름을 넣어보세요')\n",
        "  ```\n",
        "  \n",
        "3. datasets의 shuffle() 메소드와 select() 메소드를 사용하여 데이터를 추출해보세요.\n",
        "  ```python\n",
        "    dataset['train'].shuffle(seed=seed).select(range(n))\n",
        "  ```\n",
        "\n",
        "> 못 풀겠어도 걱정하지 마세요. [이 문서](https://huggingface.co/docs/datasets/main/en/loading)에서 답을 찾을 수 있어요 👀"
      ],
      "metadata": {
        "id": "KETES-Fd7ZEW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlGwqxXKkpwG"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 불러오기\n",
        "\n",
        "\n",
        "# 데이터 불러오기\n",
        "\n",
        "\n",
        "# 데이터 추출하기\n",
        "train_dataset =\n",
        "test_dataset ="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터셋에서 첫 번째 샘플 출력\n",
        "print(\"\\n학습 데이터 샘플:\")\n",
        "print(train_dataset[0])\n",
        "\n",
        "# 테스트 데이터셋에서 첫 번째 샘플 출력\n",
        "print(\"\\n테스트 데이터 샘플:\")\n",
        "print(test_dataset[0])"
      ],
      "metadata": {
        "id": "UGoWHr7U1vl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### #문제 005\n",
        "\n",
        "문제 005에서 negative `'0'`과 positive `'1'`로 인코딩된 1,000개의 학습 데이터 중에서 800개는 학습 `training` 용도로, 200개는 검증 `validation` 용도로 랜덤하게 나누어 보세요.\n",
        "\n",
        "\n",
        "> 1,000개의 데이터를 800개의 학습용과 200개의 검증용으로 나누는 이유는 모델의 효과적인 학습과 평가를 위해서입니다. 학습 데이터로 모델을 훈련시키고, 검증 데이터로 모델의 성능을 평가합니다. 이렇게 분리함으로써 모델이 학습 데이터만을 '외우는' 과적합을 방지하고, 새로운 데이터에 대한 일반화 능력을 확인할 수 있습니다. 또한, 검증 결과를 바탕으로 모델의 하이퍼파라미터를 조정하여 성능을 개선할 수 있습니다. 이는 머신러닝 모델 개발에서 중요한 과정으로, 더 견고하고 신뢰할 수 있는 모델을 만드는 데 도움이 됩니다."
      ],
      "metadata": {
        "id": "xRJ4M1gdA1cy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 HINT\n",
        "\n",
        "1. `train_test_split()` 메소드를 사용하여 학습 데이터를 8:2로 나누어보세요.\n",
        "\n",
        "2. `train_test_split()` 메소드에서 사용하는 [파라미터를 확인](https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/main_classes#datasets.Dataset.train_test_split)하고 데이터를 나누는 다양한 방법에 대해 익혀보세요.\n",
        "\n",
        "> 못 풀겠어도 걱정하지 마세요. [이 문서](https://huggingface.co/docs/datasets/process#split)에서 답을 찾을 수 있어요 👀"
      ],
      "metadata": {
        "id": "hqyq4Fkfu4pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터를 8:2 비율로 나누기\n",
        "split_dataset = # 코드를 완성해보세요.\n",
        "\n",
        "# split_dataset에서 훈련 데이터, 검증 데이터 가져오기\n",
        "train_dataset =\n",
        "valid_dataset =\n",
        "\n",
        "# 결과 확인\n",
        "print(\"학습 데이터 크기:\", len(train_dataset))\n",
        "print(\"검증 데이터 크기:\", len(valid_dataset))\n",
        "\n",
        "# 학습 데이터의 일부 샘플 출력\n",
        "print(\"\\n학습 데이터 샘플:\")\n",
        "print(train_dataset[:5])\n",
        "\n",
        "# 검증 데이터의 일부 샘플 출력\n",
        "print(\"\\n검증 데이터 샘플:\")\n",
        "print(valid_dataset[:5])"
      ],
      "metadata": {
        "id": "OYG1b1IgBUiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### #문제 006\n",
        "\n",
        "문제 004와 005에서 추출한 학습 데이터(800개), 검증 데이터(200개), 테스트 데이터(1,000개)를 사전학습된 distilbert-base-uncased 모델에 투입하기 위해, 토크나이저를 사용해 인코딩하세요.\n",
        "\n",
        "> 토크나이징, 인코딩은 텍스트를 기계가 이해할 수 있는 형태로 변환하는 과정입니다. 토큰은 텍스트의 의미 있는 가장 작은 단위로, 보통 단어나 부분 단어를 의미합니다. 토크나이징은 텍스트를 이러한 토큰들로 나누는 과정입니다. 예를 들어, \"I love NLP\"라는 문장은 \"I\", \"love\", \"NLP\"라는 토큰으로 나눌 수 있습니다.\n",
        "\n",
        "> 인코딩은 나눈 토큰들을 숫자로 변환하는 과정입니다. 각 토큰에는 사전에 정의된 어휘 사전(vocabulary)에 따라 고유한 숫자(ID)가 할당됩니다. 예를 들어, \"I\"는 1021, \"love\"는 2293, \"NLP\"는 11234로 변환될 수 있습니다. 이렇게 변환된 숫자 시퀀스를 `input_ids`라고 부르며, 이는 [1021, 2293, 11234]와 같은 형태를 가집니다. 이렇게 변환된 숫자 형태의 데이터는 기계학습 모델이 처리하기에 적합한 형태가 됩니다."
      ],
      "metadata": {
        "id": "ch4n8Awj2tS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 HINT\n",
        "\n",
        "1. Hugging Face의 DistilBertTokenizerFast 사용하여 ['distilbert-base-uncased'](https://huggingface.co/distilbert/distilbert-base-uncased) 토크나이저를 불러오세요.\n",
        "  ```python\n",
        "    from transformers import DistilBertTokenizerFast\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"불러올 토크나이저 이름을 입력하세요.\")\n",
        "  ```\n",
        "\n",
        "2. 다음의 파라미터를 사용하여 데이터를 토크나이징해보세요. 토크나이저의 [`__call__` 메소드](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__)를 직접 사용하여 텍스트를 토크나이징할 수 있어요.\n",
        "  - `padding=True`: 모든 시퀀스의 길이를 동일하게 맞춥니다.\n",
        "  - `truncation=True`: 최대 길이를 초과하는 시퀀스를 잘라냅니다.\n",
        "  - `max_length=512`: 최대 시퀀스 길이를 512로 설정합니다.\n",
        "  - `return_tensors=\"pt\"`: PyTorch 텐서 형태로 결과를 반환합니다.\n"
      ],
      "metadata": {
        "id": "5CAiay9RAiiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBERT Fast 토크나이저 로드\n",
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer ="
      ],
      "metadata": {
        "id": "9Zxm0_gC8zLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 토크나이징하기\n",
        "tokenized_train = tokenizer(train_dataset['text'], # 파라미터를 더 추가해보세요.)\n",
        "tokenized_val = tokenizer(valid_dataset['text'], # 파라미터를 더 추가해보세요.)\n",
        "tokenized_test = tokenizer(test_dataset['text'], # 파라미터를 더 추가해보세요.)"
      ],
      "metadata": {
        "id": "3UQAP2lOEoYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 확인\n",
        "print(\"학습 데이터 인풋 크기:\", len(tokenized_train['input_ids']))\n",
        "print(\"검증 데이터 인풋 크기:\", len(tokenized_val['input_ids']))\n",
        "print(\"테스트 데이터 인풋 크기:\", len(tokenized_test['input_ids']))\n",
        "\n",
        "# 0번째 텍스트의 5번째 토큰까지의 input_ids 출력\n",
        "print(\"\\n토크나이징된 학습 데이터 샘플:\", tokenized_train['input_ids'][0][:5])\n",
        "\n",
        "# 위의 결과를 디코딩하여 출력\n",
        "print(\"\\n   디코딩된 학습 데이터 샘플:\", tokenizer.decode(tokenized_train['input_ids'][0][:5]))"
      ],
      "metadata": {
        "id": "B9zLYzzPF0qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금까지 정말 수고 많으셨습니다 🤗 지난 주 보다는 조금 어려웠는데 잘 따라오셨나요?\n",
        "\n",
        "이전에 작성한 코드를 바탕으로 Dataset을 상속하는 IMDbDataset 클래스를 만들어 볼게요. 이 클래스는 IMDB 데이터셋을 효율적으로 처리하고, 다음 주에 진행할 언어 모델 파인튜닝에 사용될 거예요."
      ],
      "metadata": {
        "id": "yqmsuxIMI_MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "\n",
        "class IMDbDataset(Dataset):\n",
        "    def __init__(self, tokenized_data, labels):\n",
        "        self.input_ids = tokenized_data['input_ids'].clone().detach()\n",
        "        self.attention_mask = tokenized_data['attention_mask'].clone().detach()\n",
        "        self.labels = torch.tensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "# 원본 데이터셋에서 labels 추출\n",
        "train_labels = train_dataset['label']\n",
        "val_labels = valid_dataset['label']\n",
        "test_labels = test_dataset['label']\n",
        "\n",
        "# 클래스 인스턴스화\n",
        "train_dataset = IMDbDataset(tokenized_train, train_labels)\n",
        "val_dataset = IMDbDataset(tokenized_val, val_labels)\n",
        "test_dataset = IMDbDataset(tokenized_test, test_labels)\n",
        "\n",
        "# 결과 확인\n",
        "print(\"학습 데이터 크기:\", len(train_dataset))\n",
        "print(\"검증 데이터 크기:\", len(val_dataset))\n",
        "print(\"테스트 데이터 크기:\", len(test_dataset))\n",
        "\n",
        "# 데이터 샘플 확인\n",
        "print(\"\\n학습 데이터 샘플:\")\n",
        "sample = train_dataset[0]\n",
        "print(\"Input IDs:\", sample['input_ids'][:10], \"...\")  # 처음 10개 토큰만 출력\n",
        "print(\"Attention Mask:\", sample['attention_mask'][:10], \"...\")\n",
        "print(\"Label:\", sample['labels'])"
      ],
      "metadata": {
        "id": "rB_h9Q8qKjlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 📝 위 코드를 이해하기 위한 개념들을 아래에다 정리해보았어요.\n",
        "- `텐서(Tensor)`\n",
        "  - 숫자 데이터를 담는 다차원 배열입니다.\n",
        "  - 머신러닝과 딥러닝에서 데이터를 표현하는 기본 단위입니다.\n",
        "  - NumPy의 배열과 유사하지만, GPU에서 빠른 연산이 가능합니다.\n",
        "  - PyTorch에서는 torch.tensor()로 생성합니다.\n",
        "- `input_ids`\n",
        "  - 텍스트를 숫자로 변환한 결과입니다.\n",
        "  - 각 단어나 부분 단어(서브워드)가 고유한 정수 ID로 표현됩니다.\n",
        "  - 예: \"I love NLP\" → [101, 1045, 2293, 19204, 102]\n",
        "  - 101과 102는 각각 문장의 시작과 끝을 나타내는 특수 토큰입니다.\n",
        "- `attention_mask`\n",
        "  - 실제 입력 데이터와 패딩을 구분하는 이진 마스크입니다.\n",
        "  - 1은 실제 토큰을, 0은 패딩 토큰을 나타냅니다.\n",
        "  - 모델이 패딩된 부분을 무시하고 실제 입력에만 주목하게 합니다.\n",
        "  - 예: [1, 1, 1, 1, 1, 0, 0] (5개의 실제 토큰, 2개의 패딩)\n",
        "- `labels`\n",
        "  - 각 텍스트 샘플의 분류 결과를 나타내는 숫자입니다.\n",
        "  - 이진 분류의 경우, 보통 0과 1로 표현합니다.\n",
        "  - IMDB 데이터셋의 경우: 0은 부정적 리뷰, 1은 긍정적 리뷰\n",
        "  - 다중 분류 작업에서는 클래스 수만큼의 고유한 정수를 사용합니다."
      ],
      "metadata": {
        "id": "4AJEwo4FLDP8"
      }
    }
  ]
}